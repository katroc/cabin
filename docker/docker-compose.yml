services:
  Qwen3-4B-Instruct-2507:
    image: vllm/vllm-openai:latest
    container_name: Qwen3-4B-Instruct-2507
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: "all"
      TORCH_CUDA_ARCH_LIST: "12.0" # Adjust based on your GPU architecture
      VLLM_LOG_LEVEL: "DEBUG"
    ports:
      - "8000:8000"
    volumes:
      - ~/models/Qwen3-4B-Instruct-2507:/models/Qwen3-4B-Instruct-2507
    command:
      - --model
      - /models/Qwen3-4B-Instruct-2507
      - --served-model-name
      - Qwen3-4B-Instruct-2507
      - --max-model-len
      - "15360"
      - --gpu-memory-utilization
      - "0.55"
      - --max-num-batched-tokens
      - "16384"
      - --max-num-seqs
      - "256"
      - --enable-chunked-prefill
      - --enable-prefix-caching

  bge-m3:
    image: vllm/vllm-openai:latest
    container_name: bge-m3
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: "all"
      TORCH_CUDA_ARCH_LIST: "12.0" # Adjust based on your GPU architecture
      VLLM_LOG_LEVEL: "DEBUG"
    ports:
      - "8001:8000"
    volumes:
      - ~/models/bge-m3:/models/bge-m3
    command:
      - --model
      - /models/bge-m3
      - --served-model-name
      - bge-m3
      - --gpu-memory-utilization
      - "0.05"
      - --max-model-len
      - "2048"

  bge-reranker-v2-m3:
    image: vllm/vllm-openai:latest
    container_name: bge-reranker-v2-m3
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: "all"
      TORCH_CUDA_ARCH_LIST: "12.0" # Adjust based on your GPU architecture
      VLLM_LOG_LEVEL: "DEBUG"
    ports:
      - "8002:8000"
    volumes:
      - ~/models/bge-reranker-v2-m3:/models/bge-reranker-v2-m3
    command:
      - --model
      - /models/bge-reranker-v2-m3
      - --served-model-name
      - bge-reranker-v2-m3
      - --gpu-memory-utilization
      - "0.10"
      - --max-model-len
      - "512"

  # Qwen3-4B-AWQ-router:
  #   image: vllm/vllm-openai:latest
  #   container_name: Qwen3-4B-AWQ-router
  #   runtime: nvidia
  #   environment:
  #     NVIDIA_VISIBLE_DEVICES: "all"
  #     TORCH_CUDA_ARCH_LIST: "12.0"
  #     VLLM_LOG_LEVEL: "INFO"
  #   ports:
  #     - "8003:8000"       # incremented from 8000 â†’ 8003
  #   volumes:
  #     - ~/models/qwen3-4b-awq:/models/qwen3-4b-awq
  #   command:
  #     - --model
  #     - /models/qwen3-4b-awq
  #     - --served-model-name
  #     - Qwen3-4B-AWQ-router
  #     - --quantization
  #     - awq
  #     - --dtype
  #     - auto
  #     - --max-model-len
  #     - "1024"
  #     - --gpu-memory-utilization
  #     - "0.60"
  #     - --max-num-seqs
  #     - "16"


  chromadb:
    image: chromadb/chroma:latest
    container_name: chromadb
    environment:
      IS_PERSISTENT: "TRUE"
      PERSIST_DIRECTORY: "/chroma/chroma"
      CHROMA_SERVER_HOST: "0.0.0.0"
      CHROMA_SERVER_HTTP_PORT: "8000"
      ANONYMIZED_TELEMETRY: "false"
    ports:
      - "8100:8000"
    volumes:
      - chroma-data:/chroma/chroma
    restart: unless-stopped

volumes:
  chroma-data:
